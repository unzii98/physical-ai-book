"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[487],{4389:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"vla-robotics/capstone-project","title":"Capstone Project: The Autonomous Humanoid","description":"This final project integrates every module in the book into a single, cohesive system.","source":"@site/docs/04-vla-robotics/03-capstone-project.md","sourceDirName":"04-vla-robotics","slug":"/vla-robotics/capstone-project","permalink":"/physical-ai-book/docs/vla-robotics/capstone-project","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-book/docs/vla-robotics/cognitive-planning"}}');var i=o(4848),s=o(8453);const r={sidebar_position:3},c="Capstone Project: The Autonomous Humanoid",a={},l=[{value:"The Mission",id:"the-mission",level:2},{value:"The Execution Pipeline",id:"the-execution-pipeline",level:2},{value:"Success Criteria",id:"success-criteria",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.p,{children:"This final project integrates every module in the book into a single, cohesive system."}),"\n",(0,i.jsx)(n.h2,{id:"the-mission",children:"The Mission"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Scenario"}),": A humanoid robot stands in a simulated kitchen (Unity/Gazebo).\n",(0,i.jsx)(n.strong,{children:"Command"}),': "I\'m thirsty. Get me something to drink."']}),"\n",(0,i.jsx)(n.h2,{id:"the-execution-pipeline",children:"The Execution Pipeline"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hearing (Module 4)"}),": The robot captures your voice command and uses Whisper to transcribe it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Thinking (Module 4)"}),': The LLM analyzes the request ("thirsty" -> needs drink). It queries its memory/map and plans: ',(0,i.jsx)(n.code,{children:"Find(Drink) -> Pick(Drink) -> Bring(User)"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Seeing (Module 3)"}),': The robot uses Isaac ROS VSLAM to locate itself and NVBlox to map the room. It activates its object detection model to scan for "bottles" or "cans".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Moving (Module 3)"}),": Nav2 plans a path to the fridge, avoiding the kitchen island."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actuating (Module 1 & 2)"}),": The robot's legs walk (simulated physics) to the target. Its arm reaches out (Inverse Kinematics), grasps the bottle (collision detection), and returns to you."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"success-criteria",children:"Success Criteria"}),"\n",(0,i.jsx)(n.p,{children:"The project is considered complete when the robot can autonomously perform this sequence in simulation without human teleoperation, handling dynamic obstacles (e.g., a chair moving) gracefully."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>c});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);