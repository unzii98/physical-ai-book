"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[645],{2877:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"1. The Robotic Nervous System (ROS 2)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/physical-ai-book/docs/robotic-nervous-system/nodes-topics-services","label":"ROS 2 Nodes, Topics, and Services","docId":"robotic-nervous-system/nodes-topics-services","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/robotic-nervous-system/python-agents-rclpy","label":"Python Agents and ROS 2 Control with rclpy","docId":"robotic-nervous-system/python-agents-rclpy","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/robotic-nervous-system/urdf-humanoids","label":"Understanding URDF for Humanoids","docId":"robotic-nervous-system/urdf-humanoids","unlisted":false}],"href":"/physical-ai-book/docs/category/1-the-robotic-nervous-system-ros-2"},{"type":"category","label":"2. The Digital Twin (Gazebo & Unity)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/physical-ai-book/docs/digital-twin-sim/gazebo-physics","label":"Physics, Gravity, and Collisions in Gazebo","docId":"digital-twin-sim/gazebo-physics","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/digital-twin-sim/unity-hri","label":"High-Fidelity Rendering in Unity","docId":"digital-twin-sim/unity-hri","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/digital-twin-sim/simulating-sensors","label":"Simulating Sensors: LiDAR, Depth, and IMU","docId":"digital-twin-sim/simulating-sensors","unlisted":false}],"href":"/physical-ai-book/docs/category/2-the-digital-twin-gazebo--unity"},{"type":"category","label":"3. The AI-Robot Brain (NVIDIA Isaac)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/physical-ai-book/docs/ai-robot-brain/isaac-sim-sdg","label":"NVIDIA Isaac Sim: Photorealism & Synthetic Data","docId":"ai-robot-brain/isaac-sim-sdg","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/ai-robot-brain/isaac-ros-vslam","label":"Isaac ROS: VSLAM & Accelerated Perception","docId":"ai-robot-brain/isaac-ros-vslam","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/ai-robot-brain/nav2-humanoid","label":"Nav2: Path Planning for Humanoids","docId":"ai-robot-brain/nav2-humanoid","unlisted":false}],"href":"/physical-ai-book/docs/category/3-the-ai-robot-brain-nvidia-isaac"},{"type":"category","label":"Tutorial - Extras","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/physical-ai-book/docs/tutorial-extras/manage-docs-versions","label":"Manage Docs Versions","docId":"tutorial-extras/manage-docs-versions","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/tutorial-extras/translate-your-site","label":"Translate your site","docId":"tutorial-extras/translate-your-site","unlisted":false}],"href":"/physical-ai-book/docs/category/tutorial---extras"},{"type":"category","label":"4. Vision-Language-Action (VLA)","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/physical-ai-book/docs/vla-robotics/voice-whisper","label":"Voice-to-Action with Whisper","docId":"vla-robotics/voice-whisper","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/vla-robotics/cognitive-planning","label":"Cognitive Planning with LLMs","docId":"vla-robotics/cognitive-planning","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/vla-robotics/capstone-project","label":"Capstone Project: The Autonomous Humanoid","docId":"vla-robotics/capstone-project","unlisted":false}],"href":"/physical-ai-book/docs/category/4-vision-language-action-vla"}]},"docs":{"ai-robot-brain/isaac-ros-vslam":{"id":"ai-robot-brain/isaac-ros-vslam","title":"Isaac ROS: VSLAM & Accelerated Perception","description":"Isaac ROS is a collection of hardware-accelerated packages that bring computer vision and AI perception to ROS 2 developers. It leverages the GPU (specifically NVIDIA Jetson or discrete GPUs) to perform heavy computations that would choke a CPU.","sidebar":"tutorialSidebar"},"ai-robot-brain/isaac-sim-sdg":{"id":"ai-robot-brain/isaac-sim-sdg","title":"NVIDIA Isaac Sim: Photorealism & Synthetic Data","description":"NVIDIA Isaac Sim is a scalable robotics simulation application and synthetic data generation tool that powers photorealistic, physically accurate virtual environments to develop, test, and manage AI-based robots.","sidebar":"tutorialSidebar"},"ai-robot-brain/nav2-humanoid":{"id":"ai-robot-brain/nav2-humanoid","title":"Nav2: Path Planning for Humanoids","description":"Nav2 (ROS 2 Navigation Stack) is the industry standard for moving robots from Point A to Point B safely. While traditionally used for wheeled robots, it is adaptable for bipeds.","sidebar":"tutorialSidebar"},"digital-twin-sim/gazebo-physics":{"id":"digital-twin-sim/gazebo-physics","title":"Physics, Gravity, and Collisions in Gazebo","description":"Gazebo is a standard-bearer in robotics simulation, offering a robust physics engine that calculates rigid body dynamics, collision detection, and environmental forces like gravity and friction.","sidebar":"tutorialSidebar"},"digital-twin-sim/simulating-sensors":{"id":"digital-twin-sim/simulating-sensors","title":"Simulating Sensors: LiDAR, Depth, and IMU","description":"A digital twin is useless if the robot can\'t \\"see\\" it. We must simulate the data streams that real sensors would produce.","sidebar":"tutorialSidebar"},"digital-twin-sim/unity-hri":{"id":"digital-twin-sim/unity-hri","title":"High-Fidelity Rendering in Unity","description":"While Gazebo excels at physics, Unity (a game engine) is increasingly used in robotics for its superior visual fidelity and interactive capabilities, especially for Human-Robot Interaction (HRI) scenarios.","sidebar":"tutorialSidebar"},"robotic-nervous-system/nodes-topics-services":{"id":"robotic-nervous-system/nodes-topics-services","title":"ROS 2 Nodes, Topics, and Services","description":"ROS 2 (Robot Operating System 2) provides a flexible framework for writing robot software. Its core components\u2014Nodes, Topics, and Services\u2014enable modular and distributed development.","sidebar":"tutorialSidebar"},"robotic-nervous-system/python-agents-rclpy":{"id":"robotic-nervous-system/python-agents-rclpy","title":"Python Agents and ROS 2 Control with rclpy","description":"rclpy is the Python client library for ROS 2, enabling Python developers to interface with the ROS 2 ecosystem seamlessly. This is crucial for integrating high-level AI agents, often written in Python, with the low-level robot hardware control provided by ROS 2.","sidebar":"tutorialSidebar"},"robotic-nervous-system/urdf-humanoids":{"id":"robotic-nervous-system/urdf-humanoids","title":"Understanding URDF for Humanoids","description":"The Unified Robot Description Format (URDF) is an XML format for describing all aspects of a robot model. It is a fundamental building block in ROS 2 for representing a robot\'s kinematic and dynamic properties, visual appearance, and collision geometry.","sidebar":"tutorialSidebar"},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs.","sidebar":"tutorialSidebar"},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French.","sidebar":"tutorialSidebar"},"vla-robotics/capstone-project":{"id":"vla-robotics/capstone-project","title":"Capstone Project: The Autonomous Humanoid","description":"This final project integrates every module in the book into a single, cohesive system.","sidebar":"tutorialSidebar"},"vla-robotics/cognitive-planning":{"id":"vla-robotics/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Large Language Models (LLMs) like GPT-4, Llama 3, or specialized VLA models act as the \\"Prefrontal Cortex\\" of the robot. They bridge the gap between high-level abstract goals and low-level control primitives.","sidebar":"tutorialSidebar"},"vla-robotics/voice-whisper":{"id":"vla-robotics/voice-whisper","title":"Voice-to-Action with Whisper","description":"The interface between humans and robots is shifting from code to conversation. OpenAI Whisper provides the auditory cortex for this new interaction paradigm.","sidebar":"tutorialSidebar"}}}}')}}]);