"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[103],{3557:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>c,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"vla-robotics/voice-whisper","title":"Voice-to-Action with Whisper","description":"The interface between humans and robots is shifting from code to conversation. OpenAI Whisper provides the auditory cortex for this new interaction paradigm.","source":"@site/docs/04-vla-robotics/01-voice-whisper.md","sourceDirName":"04-vla-robotics","slug":"/vla-robotics/voice-whisper","permalink":"/physical-ai-book/docs/vla-robotics/voice-whisper","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"4. Vision-Language-Action (VLA)","permalink":"/physical-ai-book/docs/category/4-vision-language-action-vla"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/physical-ai-book/docs/vla-robotics/cognitive-planning"}}');var o=n(4848),r=n(8453);const s={sidebar_position:1},c="Voice-to-Action with Whisper",a={},l=[{value:"The Voice Pipeline",id:"the-voice-pipeline",level:2},{value:"From Text to Intent",id:"from-text-to-intent",level:2}];function h(e){const t={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"voice-to-action-with-whisper",children:"Voice-to-Action with Whisper"})}),"\n",(0,o.jsxs)(t.p,{children:["The interface between humans and robots is shifting from code to conversation. ",(0,o.jsx)(t.strong,{children:"OpenAI Whisper"})," provides the auditory cortex for this new interaction paradigm."]}),"\n",(0,o.jsx)(t.h2,{id:"the-voice-pipeline",children:"The Voice Pipeline"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Audio Capture"}),": The robot's microphone array captures raw audio."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Transcription"}),": The audio stream is sent to the Whisper model (running locally on an NVIDIA Jetson or in the cloud). Whisper is robust to accents, background noise, and technical jargon."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Text Output"}),': The model outputs a text string, e.g., "Robot, please pick up the red apple."']}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"from-text-to-intent",children:"From Text to Intent"}),"\n",(0,o.jsxs)(t.p,{children:["Transcribed text is just a string. To make it actionable, we need to extract ",(0,o.jsx)(t.strong,{children:"Intent"})," and ",(0,o.jsx)(t.strong,{children:"Slots"}),"."]}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Input"}),': "Pick up the red apple."']}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Intent"}),": ",(0,o.jsx)(t.code,{children:"PICK_UP_OBJECT"})]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Slot (Object)"}),": ",(0,o.jsx)(t.code,{children:"apple"})]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Slot (Attribute)"}),": ",(0,o.jsx)(t.code,{children:"red"})]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:"This structured data is then passed to the cognitive planner (LLM) or a state machine to trigger the appropriate ROS 2 action."})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>c});var i=n(6540);const o={},r=i.createContext(o);function s(e){const t=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:t},e.children)}}}]);